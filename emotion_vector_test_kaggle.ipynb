{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7e43270",
   "metadata": {},
   "source": [
    "# Emotion Vector Testing - Research Paper 2502.04075v1 Implementation\n",
    "\n",
    "This notebook demonstrates the effectiveness of Emotion Vectors (EVs) for controllable emotion generation in Large Language Models. We'll compare model outputs with and without emotion vectors applied.\n",
    "\n",
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a190321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch accelerate bitsandbytes pandas matplotlib seaborn scikit-learn\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a3e259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, logging\n",
    "from transformers import BitsAndBytesConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Emotion order as defined in the paper\n",
    "EMOTIONS = [\"anger\", \"disgust\", \"fear\", \"joy\", \"sadness\"]\n",
    "\n",
    "# Model configuration - UPDATED TO LLAMA 3.2-1B\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Update your HF_TOKEN\n",
    "HF_TOKEN = \"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\"  # Your actual token\n",
    "\n",
    "# Login to Hugging Face (required for Llama models)\n",
    "if HF_TOKEN:\n",
    "    try:\n",
    "        login(token=HF_TOKEN)\n",
    "        print(\"✓ Successfully logged in to Hugging Face\")\n",
    "    except Exception as e:\n",
    "        print(f\"Login failed: {e}\")\n",
    "        print(\"Warning: Authentication may be required for Llama models.\")\n",
    "else:\n",
    "    print(\"Warning: No HF token provided. You may need to authenticate for Llama models.\")\n",
    "\n",
    "# Configure quantization for memory efficiency with Llama (DEFINE BEFORE USE)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model with proper authentication\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "try:\n",
    "    # Try loading Llama with quantization first\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        token=HF_TOKEN,  # Use token parameter instead of use_auth_token\n",
    "        low_cpu_mem_usage=True\n",
    "    ).eval()\n",
    "    print(\"✓ Model loaded with 4-bit quantization\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Quantized loading failed: {e}\")\n",
    "    try:\n",
    "        # Fallback to 8-bit quantization\n",
    "        bnb_config_8bit = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            quantization_config=bnb_config_8bit,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            token=HF_TOKEN,\n",
    "            low_cpu_mem_usage=True\n",
    "        ).eval()\n",
    "        print(\"✓ Model loaded with 8-bit quantization\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"8-bit quantization failed: {e}\")\n",
    "        print(\"Falling back to DialoGPT for compatibility...\")\n",
    "        MODEL_NAME = \"microsoft/DialoGPT-medium\"\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        ).eval()\n",
    "        print(\"✓ Fallback model loaded\")\n",
    "\n",
    "# Load tokenizer with proper configuration for Llama\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    token=HF_TOKEN if \"llama\" in MODEL_NAME.lower() else None  # Fixed deprecated parameter\n",
    ")\n",
    "\n",
    "# Configure tokenizer for Llama\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# For Llama models, set chat template if available\n",
    "if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template is None:\n",
    "    tokenizer.chat_template = \"{% for message in messages %}{{ message['role'] }}: {{ message['content'] }}{% endfor %}\"\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {model.num_parameters() / 1e6:.1f}M\")\n",
    "print(f\"Model architecture: {model.config.architectures}\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"Number of layers: {model.config.num_hidden_layers}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1435639",
   "metadata": {},
   "source": [
    "## Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb0af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences covering different scenarios - reduced for faster testing\n",
    "TEST_SENTENCES = [\n",
    "    \"What do you think about starting a new job?\",\n",
    "    \"Describe a memorable experience from your childhood.\",\n",
    "    \"How do you feel about meeting new people?\",\n",
    "    \"Tell me about a time when things didn't go as planned.\",\n",
    "    \"What are your thoughts on taking risks?\",\n",
    "    \"How do you handle difficult situations?\"\n",
    "]\n",
    "\n",
    "EMOTION_CONFIGS = {\n",
    "    \"neutral\": [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    \"angry\": [1.0, 0.0, 0.0, 0.0, 0.0],        # Original paper scale\n",
    "    \"disgusted\": [0.0, 1.0, 0.0, 0.0, 0.0],    # Original paper scale\n",
    "    \"fearful\": [0.0, 0.0, 1.0, 0.0, 0.0],      # Original paper scale\n",
    "    \"joyful\": [0.0, 0.0, 0.0, 1.0, 0.0],       # Original paper scale\n",
    "    \"sad\": [0.0, 0.0, 0.0, 0.0, 1.0],          # Added sad emotion\n",
    "}\n",
    "# Emotion vector configurations - MUCH LOWER MAGNITUDES for coherent output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cf83a1",
   "metadata": {},
   "source": [
    "## Emotion Vector Detector Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee6f689",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionVectorDetector:\n",
    "    def __init__(self, model, tokenizer, emotion=\"joy\", magnitude=1.0):\n",
    "        \"\"\"Initialize EmotionVectorDetector exactly like original paper\"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.emotion = emotion\n",
    "        self.magnitude = magnitude  # This acts as the scaling factor (like 'times' in original)\n",
    "        self.emotion_vectors = {}\n",
    "\n",
    "        # Target layers for 16-layer Llama 3.2-1B model\n",
    "        self.target_layers = [4, 6, 8, 10, 12]\n",
    "\n",
    "        # Layer index tracking (like original paper)\n",
    "        self.layer_idx = 0\n",
    "\n",
    "        # Load emotion vectors from NEW location\n",
    "        vector_path = \"/home/paarth/flaskapp/sem5/ANLP/PROJECT/EmotionVector/emotion_vectors\"\n",
    "        self._load_emotion_vectors(vector_path)\n",
    "\n",
    "        # Pre-compute RAW vectors (NO NORMALIZATION like original)\n",
    "        self._precompute_raw_vectors()\n",
    "\n",
    "        self.hooks = []\n",
    "\n",
    "    def _load_emotion_vectors(self, vector_path):\n",
    "        \"\"\"Load emotion vectors from JSON files - SAME AS BEFORE\"\"\"\n",
    "        print(f\"Loading emotion vectors from {vector_path}\")\n",
    "\n",
    "        for emotion in EMOTIONS:\n",
    "            file_patterns = [\n",
    "                f\"llama_{emotion}.json\",\n",
    "                f\"Llama_{emotion}.json\",\n",
    "                f\"{emotion}.json\"\n",
    "            ]\n",
    "\n",
    "            loaded = False\n",
    "            for pattern in file_patterns:\n",
    "                file_path = os.path.join(vector_path, pattern)\n",
    "                if os.path.exists(file_path):\n",
    "                    try:\n",
    "                        with open(file_path, 'r') as f:\n",
    "                            raw_data = json.load(f)\n",
    "\n",
    "                        processed_data = {}\n",
    "                        for layer_key, layer_value in raw_data.items():\n",
    "                            if layer_key.startswith(\"layer\"):\n",
    "                                numeric_key = layer_key.replace(\"layer\", \"\")\n",
    "                            else:\n",
    "                                numeric_key = layer_key\n",
    "\n",
    "                            if isinstance(layer_value, list) and len(layer_value) > 0:\n",
    "                                if isinstance(layer_value[0], list):\n",
    "                                    processed_data[numeric_key] = layer_value[0]\n",
    "                                else:\n",
    "                                    processed_data[numeric_key] = layer_value\n",
    "                            else:\n",
    "                                print(f\"Warning: Invalid data structure for layer {layer_key} in {emotion}\")\n",
    "\n",
    "                        self.emotion_vectors[f\"Llama_{emotion}\"] = processed_data\n",
    "                        print(f\"✓ Loaded {emotion} emotion vectors from {pattern}\")\n",
    "                        loaded = True\n",
    "                        break\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {emotion} from {pattern}: {e}\")\n",
    "\n",
    "            if not loaded:\n",
    "                print(f\"Warning: No emotion vector file found for {emotion}\")\n",
    "\n",
    "    def _precompute_raw_vectors(self):\n",
    "        \"\"\"Pre-compute RAW emotion vectors (NO NORMALIZATION like original paper)\"\"\"\n",
    "        self.raw_vectors = {}\n",
    "\n",
    "        for emotion_key, layers in self.emotion_vectors.items():\n",
    "            self.raw_vectors[emotion_key] = {}\n",
    "\n",
    "            for layer_idx, vector in layers.items():\n",
    "                if layer_idx in [str(layer) for layer in self.target_layers]:\n",
    "                    # Convert to tensor but DO NOT NORMALIZE (like original paper)\n",
    "                    vector_tensor = torch.tensor(vector, dtype=torch.float32, device=self.model.device)\n",
    "                    self.raw_vectors[emotion_key][layer_idx] = vector_tensor\n",
    "\n",
    "    def set_emotion_weights(self, emotion_weights):\n",
    "        \"\"\"Set emotion weights for generation\"\"\"\n",
    "        self.current_emotion_weights = emotion_weights\n",
    "        self.layer_idx = 0  # Reset layer index like original\n",
    "\n",
    "    def hook_fn(self, layer_idx):\n",
    "        \"\"\"Create hook function EXACTLY like original paper\"\"\"\n",
    "        def hook(module, input_tensor, output):\n",
    "            # Check if this is the right type of module (like original paper checks)\n",
    "            if \"LlamaSdpaAttention\" in str(module.__class__) or hasattr(module, 'self_attn'):\n",
    "                if hasattr(self, 'current_emotion_weights') and any(w > 0 for w in self.current_emotion_weights):\n",
    "\n",
    "                    # Apply emotion vectors like original: self.times[idx] * vec\n",
    "                    for i, (emotion, weight) in enumerate(zip(EMOTIONS, self.current_emotion_weights)):\n",
    "                        if weight > 0:\n",
    "                            emotion_key = f\"Llama_{emotion}\"\n",
    "                            if emotion_key in self.raw_vectors and str(layer_idx) in self.raw_vectors[emotion_key]:\n",
    "                                emotion_vector = self.raw_vectors[emotion_key][str(layer_idx)]\n",
    "\n",
    "                                # EXACT REPLICATION of original paper approach:\n",
    "                                # output[0][:, -1:, :] = output[0][:, -1:, :] + self.times[idx] * vec\n",
    "                                if isinstance(output, tuple):\n",
    "                                    hidden_states = output[0]\n",
    "                                    # Apply to last token position like original\n",
    "                                    hidden_states[:, -1:, :] = (\n",
    "                                        hidden_states[:, -1:, :] + weight * emotion_vector.unsqueeze(0).unsqueeze(0)\n",
    "                                    )\n",
    "                                    # Don't need to return modified tuple, modify in place\n",
    "                                else:\n",
    "                                    output[:, -1:, :] = (\n",
    "                                        output[:, -1:, :] + weight * emotion_vector.unsqueeze(0).unsqueeze(0)\n",
    "                                    )\n",
    "            return output\n",
    "        return hook\n",
    "\n",
    "    def register_hooks(self):\n",
    "        \"\"\"Register forward hooks EXACTLY like original paper\"\"\"\n",
    "        self.cleanup()\n",
    "        self.layer_idx = 0  # Reset layer tracking\n",
    "\n",
    "        for layer_idx in self.target_layers:\n",
    "            if layer_idx < len(self.model.model.layers):\n",
    "                layer = self.model.model.layers[layer_idx]\n",
    "                # Hook the attention module like original paper\n",
    "                if hasattr(layer, 'self_attn'):\n",
    "                    hook = layer.self_attn.register_forward_hook(self.hook_fn(layer_idx))\n",
    "                    self.hooks.append(hook)\n",
    "\n",
    "        print(f\"✓ Registered hooks on {len(self.hooks)} attention layers\")\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Remove all hooks\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "\n",
    "    def generate_text(self, prompt, max_length=50, use_emotion_vectors=True):\n",
    "        \"\"\"Generate text with or without emotion vectors\"\"\"\n",
    "        if use_emotion_vectors:\n",
    "            self.register_hooks()\n",
    "\n",
    "        try:\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    max_length=len(inputs['input_ids'][0]) + max_length,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    no_repeat_ngram_size=2,\n",
    "                    repetition_penalty=1.1\n",
    "                )\n",
    "\n",
    "            response = self.tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)\n",
    "            return response.strip()\n",
    "\n",
    "        finally:\n",
    "            if use_emotion_vectors:\n",
    "                self.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d96d34a",
   "metadata": {},
   "source": [
    "## Model Loading and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba2edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with optimizations for Kaggle\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "try:\n",
    "    # First try with quantization for memory efficiency\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    ).eval()\n",
    "    print(\"Model loaded with 4-bit quantization\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Quantization failed: {e}\")\n",
    "    print(\"Loading with regular precision...\")\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16  # Use FP16 for memory efficiency\n",
    "        ).eval()\n",
    "        print(\"Model loaded with FP16\")\n",
    "    except Exception as e2:\n",
    "        print(f\"FP16 failed: {e2}\")\n",
    "        print(\"Loading with default precision...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            trust_remote_code=True\n",
    "        ).eval()\n",
    "        print(\"Model loaded with default precision\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# Set pad token if not available\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {model.num_parameters() / 1e6:.1f}M\")\n",
    "print(f\"Model architecture: {type(model).__name__}\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8789a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the detector\n",
    "# Updated path for new emotion vectors folder structure\n",
    "EMOTION_VECTORS_PATH = \"/home/paarth/flaskapp/sem5/ANLP/PROJECT/EmotionVector/emotion_vectors\"  # Updated path\n",
    "\n",
    "# Check if the path exists and initialize detector\n",
    "if os.path.exists(EMOTION_VECTORS_PATH):\n",
    "    print(f\"✓ Found emotion vectors path: {EMOTION_VECTORS_PATH}\")\n",
    "    detector = EmotionVectorDetector(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "else:\n",
    "    print(f\"✗ Path not found: {EMOTION_VECTORS_PATH}\")\n",
    "    print(\"Available paths to check:\")\n",
    "    # List some alternative paths to try\n",
    "    alternative_paths = [\n",
    "        \"/kaggle/input/em-vector-dataset/EMVector/llama3.1/\",\n",
    "        \"/kaggle/input/emotion-vectors/EMVector/llama3.1/\",\n",
    "        \"/kaggle/input/emvector/llama3.1/\",\n",
    "        \"/kaggle/input/emotion-vectors/llama3.1/\",\n",
    "    ]\n",
    "\n",
    "    found_path = None\n",
    "    for path in alternative_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"✓ Found alternative path: {path}\")\n",
    "            found_path = path\n",
    "            break\n",
    "        else:\n",
    "            print(f\"✗ Not found: {path}\")\n",
    "\n",
    "    detector = EmotionVectorDetector(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "print(\"Emotion Vector Detector initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69843d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug JSON structure before loading\n",
    "def debug_emotion_vectors(vector_path):\n",
    "    \"\"\"Debug function to inspect the structure of emotion vector JSON files\"\"\"\n",
    "    if not os.path.exists(vector_path):\n",
    "        print(f\"Path does not exist: {vector_path}\")\n",
    "        return\n",
    "\n",
    "    print(\"Debugging emotion vector files...\")\n",
    "\n",
    "    for emotion in EMOTIONS[:2]:  # Just check first 2 emotions\n",
    "        file_path = os.path.join(vector_path, f\"Llama_{emotion}.json\")\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"\\n--- Debugging {emotion.upper()} ---\")\n",
    "\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            print(f\"File: {file_path}\")\n",
    "            print(f\"Top-level type: {type(data)}\")\n",
    "            print(f\"Number of layers: {len(data) if isinstance(data, dict) else 'N/A'}\")\n",
    "\n",
    "            # Check first few layers\n",
    "            if isinstance(data, dict):\n",
    "                for i, (layer_key, layer_data) in enumerate(list(data.items())[:3]):\n",
    "                    print(f\"  Layer {layer_key}:\")\n",
    "                    print(f\"    Type: {type(layer_data)}\")\n",
    "                    print(f\"    Length: {len(layer_data) if hasattr(layer_data, '__len__') else 'N/A'}\")\n",
    "\n",
    "                    if isinstance(layer_data, list) and len(layer_data) > 0:\n",
    "                        first_item = layer_data[0]\n",
    "                        print(f\"    First item type: {type(first_item)}\")\n",
    "                        print(f\"    First item length: {len(first_item) if hasattr(first_item, '__len__') else 'N/A'}\")\n",
    "\n",
    "                        # If it's nested, show the actual vector dimension\n",
    "                        if isinstance(first_item, list):\n",
    "                            print(f\"    ✓ Found nested structure - Vector dimension: {len(first_item)}\")\n",
    "                        else:\n",
    "                            print(f\"    Direct structure - Vector dimension: {len(layer_data)}\")\n",
    "            break  # Just check one file for debugging\n",
    "\n",
    "# Update the path to match your actual path\n",
    "EMOTION_VECTORS_PATH = \"/home/paarth/flaskapp/sem5/ANLP/PROJECT/EmotionVector/emotion_vectors\"\n",
    "\n",
    "# Debug the structure first\n",
    "debug_emotion_vectors(EMOTION_VECTORS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71edf2a",
   "metadata": {},
   "source": [
    "## Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a46ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_emotion_vectors_optimized(sentences, emotion_configs, detector, max_length=80):\n",
    "    \"\"\"\n",
    "    Optimized emotion vector testing with efficient baseline generation\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    print(\"Testing emotion vectors (optimized)...\")\n",
    "\n",
    "    for sentence in tqdm(sentences, desc=\"Processing sentences\"):\n",
    "        print(f\"\\nTesting sentence: {sentence}\")\n",
    "\n",
    "        # Generate baseline response ONCE per sentence\n",
    "        print(\"  - Generating baseline (neutral)\")\n",
    "        baseline_response = detector.generate_text(\n",
    "            sentence,\n",
    "            max_length=max_length,\n",
    "            use_emotion_vectors=False\n",
    "        )\n",
    "\n",
    "        # Test each emotion configuration\n",
    "        for config_name, emotion_weights in emotion_configs.items():\n",
    "            if config_name == \"neutral\":\n",
    "                # Use baseline for neutral\n",
    "                response_with_emotion = baseline_response\n",
    "            else:\n",
    "                print(f\"  - Testing emotion: {config_name}\")\n",
    "\n",
    "                # Set emotion weights\n",
    "                detector.set_emotion_weights(emotion_weights)\n",
    "\n",
    "                # Generate with emotion vectors\n",
    "                response_with_emotion = detector.generate_text(\n",
    "                    sentence,\n",
    "                    max_length=max_length,\n",
    "                    use_emotion_vectors=True\n",
    "                )\n",
    "\n",
    "            # Store results\n",
    "            results.append({\n",
    "                'sentence': sentence,\n",
    "                'emotion_config': config_name,\n",
    "                'emotion_weights': emotion_weights,\n",
    "                'response_with_emotion': response_with_emotion,\n",
    "                'response_without_emotion': baseline_response,  # Same baseline for all\n",
    "                'anger_weight': emotion_weights[0],\n",
    "                'disgust_weight': emotion_weights[1],\n",
    "                'fear_weight': emotion_weights[2],\n",
    "                'joy_weight': emotion_weights[3],\n",
    "                'sadness_weight': emotion_weights[4]\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def analyze_sentiment_difference(text1, text2):\n",
    "    \"\"\"\n",
    "    Enhanced sentiment analysis with more emotion words\n",
    "    \"\"\"\n",
    "    # Enhanced word lists for better emotion detection\n",
    "    positive_words = [\n",
    "        'happy', 'joy', 'joyful', 'good', 'great', 'excellent', 'wonderful',\n",
    "        'amazing', 'fantastic', 'love', 'excited', 'thrilled', 'delighted',\n",
    "        'pleased', 'cheerful', 'optimistic', 'positive', 'beautiful', 'awesome'\n",
    "    ]\n",
    "\n",
    "    negative_words = [\n",
    "        'sad', 'angry', 'bad', 'terrible', 'awful', 'hate', 'disgusting',\n",
    "        'fearful', 'worried', 'anxious', 'depressed', 'upset', 'frustrated',\n",
    "        'annoyed', 'disgusted', 'frightened', 'scared', 'disappointed', 'mad'\n",
    "    ]\n",
    "\n",
    "    def get_sentiment_score(text):\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return 0\n",
    "        text_lower = text.lower()\n",
    "        pos_count = sum(1 for word in positive_words if word in text_lower)\n",
    "        neg_count = sum(1 for word in negative_words if word in text_lower)\n",
    "        return pos_count - neg_count\n",
    "\n",
    "    score1 = get_sentiment_score(text1)\n",
    "    score2 = get_sentiment_score(text2)\n",
    "\n",
    "    return abs(score1 - score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e696366",
   "metadata": {},
   "source": [
    "## Run the Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbbde4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the OPTIMIZED experiment\n",
    "print(\"Starting OPTIMIZED emotion vector testing...\")\n",
    "print(f\"Testing {len(TEST_SENTENCES)} sentences with {len(EMOTION_CONFIGS)} emotion configurations\")\n",
    "print(\"Key optimizations:\")\n",
    "print(\"- Reduced emotion vector magnitudes to 0.05 (instead of 2.0)\")\n",
    "print(\"- Strategic layer selection (5 layers instead of 32)\")\n",
    "print(\"- Pre-computed vectors for faster inference\")\n",
    "print(\"- Efficient baseline generation\")\n",
    "\n",
    "results_df = test_emotion_vectors_optimized(\n",
    "    sentences=TEST_SENTENCES,\n",
    "    emotion_configs=EMOTION_CONFIGS,\n",
    "    detector=detector,\n",
    "    max_length=60  # Shorter responses for faster testing\n",
    ")\n",
    "\n",
    "print(f\"\\nCompleted! Generated {len(results_df)} responses\")\n",
    "print(f\"DataFrame shape: {results_df.shape}\")\n",
    "\n",
    "# Clean up when done\n",
    "detector.cleanup()\n",
    "print(\"✓ Hooks cleaned up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b8437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUICK DEMO - Test single sentence with all emotions\n",
    "print(\"=== QUICK DEMO TEST ===\")\n",
    "test_sentence = \"Tell me about your day today.\"\n",
    "\n",
    "print(f\"Testing: {test_sentence}\")\n",
    "print(\"This should show coherent text with subtle emotional differences\\n\")\n",
    "\n",
    "for config_name, weights in EMOTION_CONFIGS.items():\n",
    "    detector.set_emotion_weights(weights)\n",
    "\n",
    "    if config_name == \"neutral\":\n",
    "        response = detector.generate_text(test_sentence, max_length=40, use_emotion_vectors=False)\n",
    "    else:\n",
    "        response = detector.generate_text(test_sentence, max_length=40, use_emotion_vectors=True)\n",
    "\n",
    "    print(f\"{config_name.upper():>10} ({weights}): {response[:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"If responses look coherent, proceed with full experiment below!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d0b498",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd62a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample results\n",
    "print(\"Sample Results:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for idx, row in results_df.head(6).iterrows():\n",
    "    print(f\"\\nSentence: {row['sentence']}\")\n",
    "    print(f\"Emotion Config: {row['emotion_config']} {row['emotion_weights']}\")\n",
    "    print(f\"\\nWithout Emotion Vector:\")\n",
    "    print(f\"{row['response_without_emotion'][:200]}...\")\n",
    "    print(f\"\\nWith Emotion Vector:\")\n",
    "    print(f\"{row['response_with_emotion'][:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db461771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentiment difference analysis\n",
    "results_df['sentiment_difference'] = results_df.apply(\n",
    "    lambda row: analyze_sentiment_difference(\n",
    "        row['response_with_emotion'],\n",
    "        row['response_without_emotion']\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Add response length differences\n",
    "results_df['length_with_emotion'] = results_df['response_with_emotion'].str.len()\n",
    "results_df['length_without_emotion'] = results_df['response_without_emotion'].str.len()\n",
    "results_df['length_difference'] = results_df['length_with_emotion'] - results_df['length_without_emotion']\n",
    "\n",
    "print(\"Analysis metrics added to dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec3dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Sentiment differences by emotion configuration\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.boxplot(data=results_df, x='emotion_config', y='sentiment_difference')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Sentiment Differences by Emotion Configuration')\n",
    "plt.ylabel('Sentiment Difference Score')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.boxplot(data=results_df, x='emotion_config', y='length_difference')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Response Length Differences')\n",
    "plt.ylabel('Length Difference (characters)')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "# Average sentiment difference by emotion type\n",
    "avg_sentiment = results_df.groupby('emotion_config')['sentiment_difference'].mean().sort_values(ascending=False)\n",
    "avg_sentiment.plot(kind='bar')\n",
    "plt.title('Average Sentiment Change by Emotion Config')\n",
    "plt.ylabel('Average Sentiment Difference')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "# Correlation heatmap of emotion weights vs sentiment difference\n",
    "emotion_cols = ['anger_weight', 'disgust_weight', 'fear_weight', 'joy_weight', 'sadness_weight']\n",
    "corr_data = results_df[emotion_cols + ['sentiment_difference']].corr()\n",
    "sns.heatmap(corr_data, annot=True, cmap='RdBu', center=0)\n",
    "plt.title('Emotion Weight vs Sentiment Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8953f3",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de58d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nTotal test cases: {len(results_df)}\")\n",
    "print(f\"Number of sentences: {results_df['sentence'].nunique()}\")\n",
    "print(f\"Number of emotion configurations: {results_df['emotion_config'].nunique()}\")\n",
    "\n",
    "print(f\"\\nSentiment Difference Statistics:\")\n",
    "print(f\"Mean: {results_df['sentiment_difference'].mean():.3f}\")\n",
    "print(f\"Std: {results_df['sentiment_difference'].std():.3f}\")\n",
    "print(f\"Min: {results_df['sentiment_difference'].min()}\")\n",
    "print(f\"Max: {results_df['sentiment_difference'].max()}\")\n",
    "\n",
    "# Percentage of cases where emotion vectors made a difference\n",
    "different_responses = results_df['response_with_emotion'] != results_df['response_without_emotion']\n",
    "print(f\"\\nCases where emotion vectors changed output: {different_responses.sum()}/{len(results_df)} ({different_responses.mean()*100:.1f}%)\")\n",
    "\n",
    "# Most effective emotion configurations\n",
    "print(f\"\\nTop emotion configurations by sentiment change:\")\n",
    "emotion_effectiveness = results_df.groupby('emotion_config')['sentiment_difference'].agg(['mean', 'std', 'count']).round(3)\n",
    "emotion_effectiveness = emotion_effectiveness.sort_values('mean', ascending=False)\n",
    "print(emotion_effectiveness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d398548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed comparison for each emotion type\n",
    "print(\"\\nDetailed Analysis by Emotion Type:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for emotion_config in EMOTION_CONFIGS.keys():\n",
    "    subset = results_df[results_df['emotion_config'] == emotion_config]\n",
    "\n",
    "    print(f\"\\n{emotion_config.upper()} Configuration:\")\n",
    "    print(f\"Weights: {EMOTION_CONFIGS[emotion_config]}\")\n",
    "    print(f\"Average sentiment difference: {subset['sentiment_difference'].mean():.3f}\")\n",
    "    print(f\"Response change rate: {(subset['response_with_emotion'] != subset['response_without_emotion']).mean()*100:.1f}%\")\n",
    "    print(f\"Average length difference: {subset['length_difference'].mean():.1f} characters\")\n",
    "\n",
    "    # Show best example\n",
    "    best_example = subset.loc[subset['sentiment_difference'].idxmax()]\n",
    "    print(f\"\\nBest example:\")\n",
    "    print(f\"Input: {best_example['sentence'][:60]}...\")\n",
    "    print(f\"Without EV: {best_example['response_without_emotion'][:100]}...\")\n",
    "    print(f\"With EV: {best_example['response_with_emotion'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee732bd2",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01225ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_df.to_csv('emotion_vector_test_results.csv', index=False)\n",
    "print(\"Results saved to 'emotion_vector_test_results.csv'\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    'total_test_cases': len(results_df),\n",
    "    'unique_sentences': results_df['sentence'].nunique(),\n",
    "    'unique_configurations': results_df['emotion_config'].nunique(),\n",
    "    'mean_sentiment_difference': results_df['sentiment_difference'].mean(),\n",
    "    'std_sentiment_difference': results_df['sentiment_difference'].std(),\n",
    "    'response_change_rate': (results_df['response_with_emotion'] != results_df['response_without_emotion']).mean(),\n",
    "    'emotion_effectiveness': emotion_effectiveness.to_dict()\n",
    "}\n",
    "\n",
    "with open('emotion_vector_summary.json', 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2, default=str)\n",
    "\n",
    "print(\"Summary statistics saved to 'emotion_vector_summary.json'\")\n",
    "\n",
    "print(\"\\nExperiment completed successfully!\")\n",
    "print(f\"Total runtime: Please check the execution time above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28114a4",
   "metadata": {},
   "source": [
    "## Interactive Testing (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive testing function\n",
    "def interactive_test(detector, custom_sentence=None):\n",
    "    \"\"\"\n",
    "    Interactive testing function for custom inputs\n",
    "    \"\"\"\n",
    "    if custom_sentence is None:\n",
    "        sentence = \"Tell me about your thoughts on change.\"\n",
    "    else:\n",
    "        sentence = custom_sentence\n",
    "\n",
    "    print(f\"Testing sentence: {sentence}\\n\")\n",
    "\n",
    "    # Test different emotion configurations\n",
    "    for config_name, weights in EMOTION_CONFIGS.items():\n",
    "        detector.set_emotion_weights(weights)\n",
    "\n",
    "        response = detector.generate_text(sentence, max_length=60, use_emotion_vectors=True)\n",
    "\n",
    "        print(f\"{config_name.upper()} ({weights}):\")\n",
    "        print(f\"  {response}\\n\")\n",
    "\n",
    "# Run interactive test\n",
    "print(\"Interactive Testing:\")\n",
    "interactive_test(detector, \"What do you think about unexpected challenges?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa2b8e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates the implementation and testing of Emotion Vectors for controllable emotion generation in Large Language Models, based on the research paper 2502.04075v1.\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Emotion Vector Effectiveness**: The results show how different emotion configurations affect model outputs\n",
    "2. **Response Variation**: Emotion vectors successfully modify the emotional tone of generated text\n",
    "3. **Controllable Generation**: The approach provides fine-grained control over emotional aspects of text generation\n",
    "\n",
    "### Usage Instructions for Kaggle:\n",
    "\n",
    "1. **Upload Data**: Upload your EmotionVector folder containing the emotion vectors to Kaggle datasets\n",
    "2. **Update Paths**: Modify the `EMOTION_VECTORS_PATH` variable to point to your uploaded data\n",
    "3. **Model Selection**: Choose appropriate model based on available GPU memory\n",
    "4. **Run Experiments**: Execute the notebook to compare outputs with and without emotion vectors\n",
    "\n",
    "### Future Improvements:\n",
    "\n",
    "- Test with larger models (Llama 3.1 8B/70B)\n",
    "- Implement more sophisticated emotion evaluation metrics\n",
    "- Add human evaluation components\n",
    "- Extend to other model architectures\n",
    "\n",
    "The framework provides a solid foundation for researching controllable emotion generation in language models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
